{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "A lot of data aren't accessible through data sets or APIs. They may exist on the Internet as Web pages, though. One way to access the data without waiting for the provider to create an API is to use a technique called Web scraping.\n",
    "\n",
    "Web scraping allows us to load a Web page into Python and extract the information we want. We can then work with the data using standard analysis tools like pandas and numpy.\n",
    "\n",
    "We'll use the requests library heavily as we learn about Web scraping. This library enables us to download a Web page. We'll also use the beautifulsoup library to extract the relevant parts of the Web page.\n",
    "\n",
    "This project was a part of the DataQuest Data Science for Python track. The project is used on a simple webpage they created.\n",
    "\n",
    "## Downloading the Webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"http://dataquestio.github.io/web-scraping-pages/simple.html\")\n",
    "content = response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Information\n",
    "\n",
    "Now we need to parse the page and extract the information we want. We'll use the BeautifulSoup library to parse the Web page with Python. This library allows us to extract tags from an HTML document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is some simple content for this page.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Initialize the parser, and pass in the content we grabbed earlier.\n",
    "parser = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# Get the body tag from the document.\n",
    "# Since we passed in the top level of the document to the parser, we need to pick a branch off of the root.\n",
    "# With BeautifulSoup, we can access branches by using tag types as attributes.\n",
    "body = parser.body\n",
    "\n",
    "# Get the p tag from the body.\n",
    "p = body.p\n",
    "\n",
    "# Print the text inside the p tag.\n",
    "# Text is a property that gets the inside text of a tag.\n",
    "print(p.text)\n",
    "\n",
    "head = parser.head\n",
    "title = head.title\n",
    "title_text = title.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tag type is not always a very robust way to parse a document. It's usually better to be more explicit by using the find_all method. This method will find all occurrences of a tag in the current element, and return a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is some simple content for this page.\n"
     ]
    }
   ],
   "source": [
    "parser = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# Get a list of all occurrences of the body tag in the element.\n",
    "body = parser.find_all(\"body\")\n",
    "\n",
    "# Get the paragraph tag.\n",
    "p = body[0].find_all(\"p\")\n",
    "\n",
    "# Get the text.\n",
    "print(p[0].text)\n",
    "\n",
    "head = parser.find_all(\"head\")\n",
    "title = head[0].find_all(\"title\")\n",
    "title_text = title[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping with ID attributes\n",
    "\n",
    "HTML allows elements to have IDs. Because they are unique, we can use an ID to refer to a specific element. We will get the text of the second paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                First paragraph.\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "# Get the page content and set up a new parser.\n",
    "response = requests.get(\"http://dataquestio.github.io/web-scraping-pages/simple_ids.html\")\n",
    "content = response.content\n",
    "parser = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# Pass in the ID attribute to only get the element with that specific ID.\n",
    "first_paragraph = parser.find_all(\"p\", id=\"first\")[0]\n",
    "print(first_paragraph.text)\n",
    "second_paragraph_text = parser.find_all(\"p\", id = \"second\")[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping with Class attributes\n",
    "\n",
    "In HTML, elements can also have classes. Classes aren't globally unique. In other words, many different elements belong to the same class, usually because they share a common purpose or characteristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                First paragraph.\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "# Get the website that contains classes.\n",
    "response = requests.get(\"http://dataquestio.github.io/web-scraping-pages/simple_classes.html\")\n",
    "content = response.content\n",
    "parser = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# Get the first inner paragraph.\n",
    "# Find all the paragraph tags with the class inner-text.\n",
    "# Then, take the first element in that list.\n",
    "first_inner_paragraph = parser.find_all(\"p\", class_=\"inner-text\")[0]\n",
    "print(first_inner_paragraph.text)\n",
    "\n",
    "second_inner_paragraph_text = parser.find_all(\"p\", class_=\"inner-text\")[1].text\n",
    "\n",
    "first_outer_paragraph_text = parser.find_all(\"p\", class_=\"outer-text\")[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping with CSS\n",
    "\n",
    "We can use BeautifulSoup's .select method to work with CSS selectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                First paragraph.\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "# Get the website that contains classes and IDs.\n",
    "response = requests.get(\"http://dataquestio.github.io/web-scraping-pages/ids_and_classes.html\")\n",
    "content = response.content\n",
    "parser = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# Select all of the elements that have the first-item class.\n",
    "first_items = parser.select(\".first-item\")\n",
    "\n",
    "# Print the text of the first paragraph (the first element with the first-item class).\n",
    "print(first_items[0].text)\n",
    "\n",
    "first_outer_text = parser.select(\".outer-text\")[0].text\n",
    "second_text = parser.select(\"#second\")[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seahawks turnovers count: 1\n",
      "Patriots total plays count: 72\n",
      "Seahawks total yards count: 396\n"
     ]
    }
   ],
   "source": [
    "# Get the Superbowl box score data.\n",
    "response = requests.get(\"http://dataquestio.github.io/web-scraping-pages/2014_super_bowl.html\")\n",
    "content = response.content\n",
    "parser = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# Find the number of turnovers the Seahawks committed.\n",
    "turnovers = parser.select(\"#turnovers\")[0]\n",
    "seahawks_turnovers = turnovers.select(\"td\")[1]\n",
    "seahawks_turnovers_count = seahawks_turnovers.text\n",
    "print(\"Seahawks turnovers count: {}\".format(seahawks_turnovers_count))\n",
    "\n",
    "patriots_total_plays_count = parser.select(\"#total-plays\")[0].select(\"td\")[2].text\n",
    "seahawks_total_yards_count = parser.select(\"#total-yards\")[0].select(\"td\")[1].text\n",
    "\n",
    "print(\"Patriots total plays count: {}\".format(patriots_total_plays_count))\n",
    "print(\"Seahawks total yards count: {}\".format(seahawks_total_yards_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
